{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c60fe-dc32-42be-b440-2df6b9c439bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f65386a-ba24-4277-b430-ed5c97c81e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"vllm[cuda118]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5357823d-8cea-40b4-87ef-6105964ed5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "\u001b[33m  WARNING: The script pymupdf is installed in '/home/jupyter-wem220/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pymupdf-1.25.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1e41bf5-84fb-4aa3-8327-b83035b216f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "\u001b[33m  WARNING: The script tabulate is installed in '/home/jupyter-wem220/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed tabulate-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0e8b96a-40b5-4c79-b642-6416922a4021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/tljh/user/lib/python3.9/site-packages (from accelerate) (1.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/lib/python3.9/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/tljh/user/lib/python3.9/site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /opt/tljh/user/lib/python3.9/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.local/lib/python3.9/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.local/lib/python3.9/site-packages (from accelerate) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.9/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/tljh/user/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: requests in /opt/tljh/user/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/tljh/user/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/tljh/user/lib/python3.9/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: networkx in /opt/tljh/user/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.9/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.9/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/tljh/user/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/tljh/user/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.1)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory, accelerate-launch and accelerate-merge-weights are installed in '/home/jupyter-wem220/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed accelerate-1.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6646574-4e48-4d5b-aa00-aa149ffa10fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pdfminer.six==20250327 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in ./.local/lib/python3.9/site-packages (from pdfplumber) (11.1.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/tljh/user/lib/python3.9/site-packages (from pdfminer.six==20250327->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/tljh/user/lib/python3.9/site-packages (from pdfminer.six==20250327->pdfplumber) (38.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/tljh/user/lib/python3.9/site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/tljh/user/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.20)\n",
      "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "\u001b[33m  WARNING: The script pypdfium2 is installed in '/home/jupyter-wem220/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script pdfplumber is installed in '/home/jupyter-wem220/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pdfminer.six-20250327 pdfplumber-0.11.6 pypdfium2-4.30.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28dc9d7f-8acd-47d7-a553-cfbac889a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import fitz\n",
    "def extract_text_pymupdf(pdf_path):\n",
    "    \"\"\"Extracts text from a structured (non-scanned) PDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f185a30f-c0df-4b70-9011-67fc63076430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_pdf(pdf_path):\n",
    "    \"\"\"Extracts tables from a PDF and converts them into DataFrames.\"\"\"\n",
    "    tables = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            table = page.extract_table()\n",
    "            if table:\n",
    "                df = pd.DataFrame(table[1:], columns=table[0])  # First row as headers\n",
    "                tables.append(df)\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80618659-a0f5-45ac-a2b6-6c5aa320747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def extract_images_pdf(pdf_path, save_folder=\"images\"):\n",
    "    \"\"\"Extracts embedded images (not full pages) from a PDF and saves them as PNG files.\"\"\"\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    image_paths = []\n",
    "\n",
    "    for page_index in range(len(doc)):\n",
    "        page = doc[page_index]\n",
    "        images = page.get_images(full=True)\n",
    "        for img_index, img in enumerate(images):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            filename = f\"{save_folder}/page{page_index+1}_img{img_index+1}.{image_ext}\"\n",
    "            image.save(filename)\n",
    "            image_paths.append(filename)\n",
    "\n",
    "    return image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f042f34-ff4f-4b30-9ccb-7d0a5f4f778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_webpage(url):\n",
    "    \"\"\"Extracts text from an article webpage.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    text = \"\\n\".join([p.get_text(strip=True) for p in soup.find_all(\"p\")])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06a9b5a1-8035-4309-9e05-fb7ff1545dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_webpage(url):\n",
    "    \"\"\"Extracts tables from a webpage and converts them into DataFrames.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    tables = []\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        rows = []\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cells = [cell.get_text(strip=True) for cell in row.find_all([\"td\", \"th\"])]\n",
    "            rows.append(cells)\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows[1:], columns=rows[0])  # First row as headers\n",
    "            tables.append(df)\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95cbc462-81aa-44af-88bf-76af3c73d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_webpage(url, save_folder=\"images\"):\n",
    "    \"\"\"Extracts images from a webpage and saves them locally.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    images = []\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    for i, img in enumerate(soup.find_all(\"img\")):\n",
    "        img_url = img.get(\"src\")\n",
    "        if img_url and img_url.startswith((\"http\", \"//\")):\n",
    "            img_url = img_url if img_url.startswith(\"http\") else \"https:\" + img_url\n",
    "            try:\n",
    "                img_data = requests.get(img_url).content\n",
    "                img_filename = os.path.join(save_folder, f\"image_{i+1}.jpg\")\n",
    "                with open(img_filename, \"wb\") as f:\n",
    "                    f.write(img_data)\n",
    "                images.append(img_filename)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {img_url}: {e}\")\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58592f16-25de-4acd-ae28-52ff867560ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(source):\n",
    "    \"\"\"Extracts text from PDF or web URL.\"\"\"\n",
    "    if source.endswith(\".pdf\"):\n",
    "        return extract_text_pymupdf(source)  # Extract from PDF\n",
    "    elif source.startswith(\"http\"):\n",
    "        return extract_text_webpage(source)  # Extract from web page\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Provide a PDF or URL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae58d91d-ed42-4f30-9c84-57fd3b531fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables(source):\n",
    "    \"\"\"Extracts tables from a PDF or web URL and converts them into DataFrames.\"\"\"\n",
    "    if source.endswith(\".pdf\"):\n",
    "        return extract_tables_pdf(source)  # Extract from PDF\n",
    "    elif source.startswith(\"http\"):\n",
    "        return extract_tables_webpage(source)  # Extract from web page\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Provide a PDF or URL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2beb10e-109e-4283-b63e-be900e2cd5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(source, save_folder=\"images\"):\n",
    "    \"\"\"Extracts images from PDFs or web pages and saves them locally.\"\"\"\n",
    "    if source.endswith(\".pdf\"):\n",
    "        return extract_images_pdf(source, save_folder)  # Extract from PDF\n",
    "    elif source.startswith(\"http\"):\n",
    "        return extract_images_webpage(source, save_folder)  # Extract from web page\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Provide a PDF or URL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0954e48-734a-4609-9ad1-2ffc4b2fd09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def extract_all(sources):\n",
    "    \"\"\"Extracts text, tables, and images from multiple PDFs and web pages.\"\"\"\n",
    "    combined_text = []\n",
    "    combined_tables = []\n",
    "    combined_images = []\n",
    "    source_references = []  # Store source details\n",
    "\n",
    "    for source in sources:\n",
    "        text = extract_text(source)\n",
    "        tables = extract_tables(source)\n",
    "        images = extract_images(source)\n",
    "\n",
    "        combined_text.append(text)\n",
    "        combined_tables.extend(tables)\n",
    "        combined_images.extend(images)\n",
    "        source_references.append(source)\n",
    "\n",
    "    return combined_text, combined_tables, combined_images, source_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72d31094-e81b-400f-8f92-6e64efe32a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def generate_lesson_from_extracted_data(sources):\n",
    "    \"\"\"\n",
    "    Generates a lesson from extracted text, tables, and image filenames.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Extract all content\n",
    "    all_texts, all_tables, all_images, all_sources = extract_all(sources)\n",
    "\n",
    "    # Step 2: Convert tables to Markdown\n",
    "    def table_to_markdown(tables):\n",
    "        markdown = []\n",
    "        for df in tables:\n",
    "            try:\n",
    "                markdown.append(df.to_markdown(index=False))\n",
    "            except Exception as e:\n",
    "                markdown.append(f\"*Table rendering failed: {e}*\")\n",
    "        return \"\\n\\n\".join(markdown)\n",
    "\n",
    "    tables_markdown = table_to_markdown(all_tables)\n",
    "\n",
    "    # Step 3: Combine and truncate extracted text\n",
    "    combined_text = \"\\n\\n\".join(all_texts)[:3000]  # Safe chunk for token limit\n",
    "\n",
    "    # Step 4: Format image markdown\n",
    "    image_placeholders = \"\\n\".join([\n",
    "        f\"![image{i+1}]({img})\" for i, img in enumerate(all_images)\n",
    "    ]) if all_images else \"*No images extracted.*\"\n",
    "\n",
    "    # Step 5: Prompt assembly\n",
    "    prompt = f\"\"\"\n",
    "You are an educational content generator. Create a structured lesson based on the text, tables, and images below.\n",
    "\n",
    "### Text:\n",
    "{combined_text}\n",
    "\n",
    "### Tables (Markdown format):\n",
    "{tables_markdown}\n",
    "\n",
    "### Images (filenames):\n",
    "{', '.join(all_images)}\n",
    "\n",
    "### Instructions:\n",
    "- Use the provided text to write an engaging lesson with sections and subheadings.\n",
    "- Include **relevant tables** inline.\n",
    "- Reference each image **in context** using Markdown syntax like `![image1](images/pdf_page_11.png)` and explain what it shows.\n",
    "- If you don’t know where an image fits, include it at the end under a section called “📸 Visuals”.\n",
    "\n",
    "Generate the full lesson below:\n",
    "\"\"\"\n",
    "\n",
    "    # Step 6: Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    # Step 7: Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.to(model.device)\n",
    "\n",
    "    # Step 8: Generate output\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "    generated_ids = output.sequences[0][input_ids.shape[-1]:]\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53d48992-a508-4395-9868-599b4ca03c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def render_output_with_images_and_tables(lesson: str, images: list, tables: list) -> str:\n",
    "    \"\"\"\n",
    "    Replaces image and table references with actual markdown content for proper rendering.\n",
    "    \"\"\"\n",
    "\n",
    "    rendered = lesson\n",
    "    used_images = set()\n",
    "\n",
    "    # Replace image placeholders like ![image1](...) with actual image paths\n",
    "    for i, img_path in enumerate(images):\n",
    "        pattern = rf\"!\\[image{i+1}\\]\\([^\\)]*\\)\"  # matches ![image1](anything)\n",
    "        replacement = f\"![image{i+1}]({img_path})\"\n",
    "        if re.search(pattern, rendered):\n",
    "            rendered = re.sub(pattern, replacement, rendered)\n",
    "            used_images.add(i)\n",
    "\n",
    "    # Replace [Table X] placeholders with markdown table content\n",
    "    for i, table_md in enumerate(tables):\n",
    "        tag = f\"[Table {i+1}]\"\n",
    "        if tag in rendered:\n",
    "            rendered = rendered.replace(tag, f\"\\n\\n{table_md}\\n\\n\")\n",
    "\n",
    "    # Append any unused images at the end\n",
    "    unused_images = [i for i in range(len(images)) if i not in used_images]\n",
    "    if unused_images:\n",
    "        rendered += \"\\n\\n---\\n\\n### 📸 Additional Visuals\\n\"\n",
    "        for i in unused_images:\n",
    "            rendered += f\"\\n![image{i+1}]({images[i]})\\n\"\n",
    "\n",
    "    return rendered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32225b50-ee3a-4cef-8a93-8f7135b833a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPImage, display\n",
    "import os\n",
    "\n",
    "def display_images(images):\n",
    "    for img_path in images:\n",
    "        if os.path.exists(img_path):\n",
    "            print(f\"📸 {os.path.basename(img_path)}\")\n",
    "            display(IPImage(filename=img_path))\n",
    "        else:\n",
    "            print(f\"⚠️ Could not find image: {img_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3b89fe3-2e82-4c62-ae68-3312627f3957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2f54c7a0664566b7ee913ea081747f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📚 Learning Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\n",
      "=============================================================================================================\n",
      "\n",
      "Introduction\n",
      "------------\n",
      "\n",
      "In recent years, deep learning has achieved great success in computer vision tasks, largely thanks to the availability of large datasets and the development of convolutional neural networks (CNNs). However, most of these works focus on supervised learning, where the network is trained to predict a specific label for a given input. In this lesson, we will explore a different approach called unsupervised representation learning, which aims to learn a good representation of the input data without any prior knowledge of the output. We will use the Deep Convolutional Generative Adversarial Networks (DCGANs) proposed by Radford et al. (2016) as a case study.\n",
      "\n",
      "📈 Architecture of DCGAN\n",
      "------------------------\n",
      "\n",
      "The architecture of DCGAN consists of two main components: the generator and the discriminator. The generator takes a random noise vector as input and produces a synthetic image. The discriminator takes an image (either real or synthetic) as input and outputs a probability that the image is real. The two components are trained simultaneously, with the generator trying to produce images that are indistinguishable from real images, and the discriminator trying to correctly classify the images as real or synthetic.\n",
      "\n",
      "| Model                  | Accuracy   | Accuracy(400perclass)   | max#offeaturesunits   |\n",
      "|:-----------------------|:-----------|:------------------------|:----------------------|\n",
      "| 1LayerK-means          | 80.6%      | 63.7%(±0.7%)            | 4800                  |\n",
      "| 3LayerK-meansLearnedRF | 82.0%      | 70.7%(±0.7%)            | 3200                  |\n",
      "| ViewInvariantK-means   | 81.9%      | 72.6%(±0.7%)            | 6400                  |\n",
      "| ExemplarCNN            | 84.3%      | 77.4%(±0.2%)            | 1024                  |\n",
      "| DCGAN(ours)+L2-SVM     | 82.8%      | 73.8%(±0.4%)            | 512                   |\n",
      "\n",
      "🤖 Training DCGAN\n",
      "------------------\n",
      "\n",
      "The training process of DCGAN involves optimizing the generator and discriminator networks simultaneously. The generator is optimized using a combination of adversarial loss and reconstruction loss, while the discriminator is optimized using a binary cross-entropy loss. The authors of the paper propose several constraints on the architectural topology of the generator and discriminator networks to make them stable to train in most settings.\n",
      "\n",
      "| Model                                | errorrate          |\n",
      "|:-------------------------------------|:-------------------|\n",
      "| KNN                                  | 77.93%             |\n",
      "| TSVM                                 | 66.55%             |\n",
      "| M1+KNN                               | 65.63%             |\n",
      "| M1+TSVM                              | 54.33%             |\n",
      "| M1+M2                                | 36.02%             |\n",
      "| SWWAEwithoutdropout                  | 27.83%             |\n",
      "| SWWAEwithdropout                     | 23.56%             |\n",
      "| DCGAN(ours)+L2-SVM                   | 22.48%             |\n",
      "| SupervisedCNNwiththesamearchitecture | 28.87%(validation) |\n",
      "\n",
      "📊 Visualizing the Learned Features\n",
      "-----------------------------------\n",
      "\n",
      "To visualize the learned features, the authors of the paper use t-SNE to reduce the dimensionality of the feature space. The resulting visualization shows that the generator learns a hierarchy of representations from object parts to scenes in both the generator and discriminator.\n",
      "\n",
      "📸 Visuals\n",
      "------------\n",
      "\n",
      "Here are some of the images generated by the DCGAN:\n",
      "\n",
      "images/page4_img1.png, images/page5_img1.png, images/page5_img2.png, images/page8_img1.png, images/page9_img1.png, images/page9_img2.png,\n",
      "\n",
      "---\n",
      "\n",
      "### 📸 Additional Visuals\n",
      "\n",
      "![image1](images/page4_img1.png)\n",
      "\n",
      "![image2](images/page5_img1.png)\n",
      "\n",
      "![image3](images/page5_img2.png)\n",
      "\n",
      "![image4](images/page8_img1.png)\n",
      "\n",
      "![image5](images/page9_img1.png)\n",
      "\n",
      "![image6](images/page9_img2.png)\n",
      "\n",
      "![image7](images/page10_img1.png)\n",
      "\n",
      "![image8](images/page11_img1.png)\n",
      "\n",
      "![image9](images/page14_img1.png)\n",
      "\n",
      "![image10](images/page15_img1.png)\n",
      "\n",
      "![image11](images/page16_img1.png)\n",
      "\n",
      "📸 page4_img1.png\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(lesson_final \u001b[38;5;28;01mif\u001b[39;00m lesson_final\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ No content was generated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Display the actual images inline\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mdisplay_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m, in \u001b[0;36mdisplay_images\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(img_path):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📸 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(img_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     display(\u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ Could not find image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "pdf_path = \"1511.06434v2.pdf\"\n",
    "sources = [pdf_path]\n",
    "text, tables, images, source_refs = extract_all(sources)\n",
    "\n",
    "# Generate the base lesson\n",
    "lesson = generate_lesson_from_extracted_data(sources)\n",
    "\n",
    "# Convert tables to markdown if needed\n",
    "table_blocks = [df.to_markdown(index=False) for df in tables]\n",
    "\n",
    "# Inject images and tables into the lesson\n",
    "# After lesson generation\n",
    "lesson_final = render_output_with_images_and_tables(lesson, images, table_blocks)\n",
    "\n",
    "# Display the lesson text\n",
    "print(lesson_final if lesson_final.strip() else \"❌ No content was generated.\")\n",
    "\n",
    "# Display the actual images inline\n",
    "display(display_images(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e1db10b-b06e-4cc3-8b9f-39cad2cf0281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6270bef058574a81966ceadfaff4997c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9427eb8f05fd4b74aad178b6c268fc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Instruction:', layout=Layout(width='100%'), placeholder='Enter inst…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# --- Assume previous lesson is already generated ---\n",
    "current_lesson = lesson  # from generate_lesson_from_extracted_data(sources)\n",
    "\n",
    "# --- Load model if not already loaded ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# --- Reprompt function ---\n",
    "def reprompt_lesson(original_lesson: str, instruction: str, temperature: float = 0.7) -> str:\n",
    "    prompt = f\"{instruction.strip()}\\n\\nOriginal Lesson Plan:\\n{original_lesson.strip()}\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=temperature,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "    generated_ids = output.sequences[0][input_ids.shape[-1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# --- Widgets ---\n",
    "instruction_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter instruction or type \"approved\"',\n",
    "    description='Instruction:',\n",
    "    layout=widgets.Layout(width='100%')\n",
    ")\n",
    "\n",
    "submit_button = widgets.Button(description=\"Submit\", button_style='success')\n",
    "output_box = widgets.Output()\n",
    "\n",
    "# --- Logic to handle button click ---\n",
    "def on_submit_clicked(b):\n",
    "    global current_lesson\n",
    "    with output_box:\n",
    "        clear_output(wait=True)\n",
    "        instruction = instruction_box.value.strip()\n",
    "        if instruction.lower() == \"approved\":\n",
    "            print(\"✅ Lesson approved by the teacher.\")\n",
    "        else:\n",
    "            # Generate and display revised lesson\n",
    "            revised_lesson = reprompt_lesson(current_lesson, instruction)\n",
    "            current_lesson = revised_lesson  # update for next iteration\n",
    "            revised_final = render_output_with_images_and_tables(revised_lesson, images, table_blocks)\n",
    "            print(\"📝 Revised Lesson:\\n\")\n",
    "            print(revised_final)\n",
    "\n",
    "submit_button.on_click(on_submit_clicked)\n",
    "\n",
    "# --- Display the input and button ---\n",
    "display(widgets.VBox([instruction_box, submit_button, output_box]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a9711-4eeb-4aca-a3c7-448f545faa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, redirect, send_file\n",
    "from model_utils import process_pdf, generate_lesson, reprompt_lesson\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "UPLOAD_FOLDER = \"static/uploads\"\n",
    "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    lesson = \"\"\n",
    "    if request.method == \"POST\":\n",
    "        file = request.files[\"pdf\"]\n",
    "        instruction = request.form.get(\"instruction\", \"\")\n",
    "        filepath = os.path.join(UPLOAD_FOLDER, file.filename)\n",
    "        file.save(filepath)\n",
    "\n",
    "        # Process and generate lesson\n",
    "        text, tables, images = process_pdf(filepath)\n",
    "        if \"lesson\" not in request.form:\n",
    "            lesson = generate_lesson(text, tables, images)\n",
    "        else:\n",
    "            lesson = reprompt_lesson(request.form[\"lesson\"], instruction)\n",
    "\n",
    "        return render_template(\"index.html\", lesson=lesson, filename=file.filename)\n",
    "\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6987e-7c86-425c-aa31-272e7a4a6b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc588e4-f6be-4f0c-b128-9a6b985c3330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80ef3a7-607b-4130-a27e-813fce0c01d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ba2b7-5ea4-4c26-aea1-7aff23d8b72a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
