{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c60fe-dc32-42be-b440-2df6b9c439bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f65386a-ba24-4277-b430-ed5c97c81e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"vllm[cuda118]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5357823d-8cea-40b4-87ef-6105964ed5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "\u001b[33m  WARNING: The script pymupdf is installed in '/home/jupyter-wem220/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pymupdf-1.25.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1e41bf5-84fb-4aa3-8327-b83035b216f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "\u001b[33m  WARNING: The script tabulate is installed in '/home/jupyter-wem220/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed tabulate-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0e8b96a-40b5-4c79-b642-6416922a4021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/tljh/user/lib/python3.9/site-packages (from accelerate) (1.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/lib/python3.9/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/tljh/user/lib/python3.9/site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /opt/tljh/user/lib/python3.9/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.local/lib/python3.9/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.local/lib/python3.9/site-packages (from accelerate) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.9/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/tljh/user/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: requests in /opt/tljh/user/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/tljh/user/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/tljh/user/lib/python3.9/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: networkx in /opt/tljh/user/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.9/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.9/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/tljh/user/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/tljh/user/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.1)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory, accelerate-launch and accelerate-merge-weights are installed in '/home/jupyter-wem220/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed accelerate-1.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6646574-4e48-4d5b-aa00-aa149ffa10fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pdfminer.six==20250327 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in ./.local/lib/python3.9/site-packages (from pdfplumber) (11.1.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/tljh/user/lib/python3.9/site-packages (from pdfminer.six==20250327->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/tljh/user/lib/python3.9/site-packages (from pdfminer.six==20250327->pdfplumber) (38.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/tljh/user/lib/python3.9/site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/tljh/user/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.20)\n",
      "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "\u001b[33m  WARNING: The script pypdfium2 is installed in '/home/jupyter-wem220/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script pdfplumber is installed in '/home/jupyter-wem220/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pdfminer.six-20250327 pdfplumber-0.11.6 pypdfium2-4.30.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28dc9d7f-8acd-47d7-a553-cfbac889a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import fitz\n",
    "def extract_text_pymupdf(pdf_path):\n",
    "    \"\"\"Extracts text from a structured (non-scanned) PDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f185a30f-c0df-4b70-9011-67fc63076430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_pdf(pdf_path):\n",
    "    \"\"\"Extracts tables from a PDF and converts them into DataFrames.\"\"\"\n",
    "    tables = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            table = page.extract_table()\n",
    "            if table:\n",
    "                df = pd.DataFrame(table[1:], columns=table[0])  # First row as headers\n",
    "                tables.append(df)\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80618659-a0f5-45ac-a2b6-6c5aa320747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_pdf(pdf_path, save_folder=\"images\"):\n",
    "    \"\"\"Extracts rasterized page images from a PDF and saves them as PNG files.\"\"\"\n",
    "    import os\n",
    "    import pdfplumber\n",
    "\n",
    "    images = []\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            img = page.to_image(resolution=150).original  # PIL Image object\n",
    "            img_filename = os.path.join(save_folder, f\"pdf_page_{i+1}.png\")\n",
    "            img.save(img_filename, \"PNG\")\n",
    "            images.append(img_filename)\n",
    "\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f042f34-ff4f-4b30-9ccb-7d0a5f4f778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_webpage(url):\n",
    "    \"\"\"Extracts text from an article webpage.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    text = \"\\n\".join([p.get_text(strip=True) for p in soup.find_all(\"p\")])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06a9b5a1-8035-4309-9e05-fb7ff1545dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_webpage(url):\n",
    "    \"\"\"Extracts tables from a webpage and converts them into DataFrames.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    tables = []\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        rows = []\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cells = [cell.get_text(strip=True) for cell in row.find_all([\"td\", \"th\"])]\n",
    "            rows.append(cells)\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows[1:], columns=rows[0])  # First row as headers\n",
    "            tables.append(df)\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95cbc462-81aa-44af-88bf-76af3c73d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_webpage(url, save_folder=\"images\"):\n",
    "    \"\"\"Extracts images from a webpage and saves them locally.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    images = []\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    for i, img in enumerate(soup.find_all(\"img\")):\n",
    "        img_url = img.get(\"src\")\n",
    "        if img_url and img_url.startswith((\"http\", \"//\")):\n",
    "            img_url = img_url if img_url.startswith(\"http\") else \"https:\" + img_url\n",
    "            try:\n",
    "                img_data = requests.get(img_url).content\n",
    "                img_filename = os.path.join(save_folder, f\"image_{i+1}.jpg\")\n",
    "                with open(img_filename, \"wb\") as f:\n",
    "                    f.write(img_data)\n",
    "                images.append(img_filename)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {img_url}: {e}\")\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58592f16-25de-4acd-ae28-52ff867560ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(source):\n",
    "    \"\"\"Extracts text from PDF or web URL.\"\"\"\n",
    "    if source.endswith(\".pdf\"):\n",
    "        return extract_text_pymupdf(source)  # Extract from PDF\n",
    "    elif source.startswith(\"http\"):\n",
    "        return extract_text_webpage(source)  # Extract from web page\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Provide a PDF or URL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae58d91d-ed42-4f30-9c84-57fd3b531fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables(source):\n",
    "    \"\"\"Extracts tables from a PDF or web URL and converts them into DataFrames.\"\"\"\n",
    "    if source.endswith(\".pdf\"):\n",
    "        return extract_tables_pdf(source)  # Extract from PDF\n",
    "    elif source.startswith(\"http\"):\n",
    "        return extract_tables_webpage(source)  # Extract from web page\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Provide a PDF or URL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2beb10e-109e-4283-b63e-be900e2cd5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(source, save_folder=\"images\"):\n",
    "    \"\"\"Extracts images from PDFs or web pages and saves them locally.\"\"\"\n",
    "    if source.endswith(\".pdf\"):\n",
    "        return extract_images_pdf(source, save_folder)  # Extract from PDF\n",
    "    elif source.startswith(\"http\"):\n",
    "        return extract_images_webpage(source, save_folder)  # Extract from web page\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Provide a PDF or URL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0954e48-734a-4609-9ad1-2ffc4b2fd09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-wem220/.local/lib/python3.9/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "2025-04-03 01:48:44.805702: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-03 01:48:44.821526: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-03 01:48:44.841753: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-03 01:48:44.848090: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-03 01:48:44.862575: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-03 01:48:45.744318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def extract_all(sources):\n",
    "    \"\"\"Extracts text, tables, and images from multiple PDFs and web pages.\"\"\"\n",
    "    combined_text = []\n",
    "    combined_tables = []\n",
    "    combined_images = []\n",
    "    source_references = []  # Store source details\n",
    "\n",
    "    for source in sources:\n",
    "        text = extract_text(source)\n",
    "        tables = extract_tables(source)\n",
    "        images = extract_images(source)\n",
    "\n",
    "        combined_text.append(text)\n",
    "        combined_tables.extend(tables)\n",
    "        combined_images.extend(images)\n",
    "        source_references.append(source)\n",
    "\n",
    "    return combined_text, combined_tables, combined_images, source_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d31094-e81b-400f-8f92-6e64efe32a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def generate_lesson_from_extracted_data(sources):\n",
    "    \"\"\"Extracts data from multiple sources and generates an AI-powered lesson with embedded tables and image placeholders.\"\"\"\n",
    "\n",
    "    # Step 1: Extract content\n",
    "    all_texts, all_tables, all_images, all_sources = extract_all(sources)\n",
    "\n",
    "    # Step 2: Convert tables to Markdown\n",
    "    def table_to_markdown(tables):\n",
    "        markdown_tables = []\n",
    "        for df in tables:\n",
    "            try:\n",
    "                markdown_tables.append(df.to_markdown(index=False))\n",
    "            except Exception as e:\n",
    "                markdown_tables.append(f\"*Table could not be rendered: {e}*\")\n",
    "        return \"\\n\\n\".join(markdown_tables)\n",
    "\n",
    "    tables_markdown = table_to_markdown(all_tables)\n",
    "\n",
    "    # Step 3: Prepare combined text\n",
    "    combined_text = \"\\n\\n\".join(all_texts)[:4000]  # Truncate to fit token limits\n",
    "\n",
    "    # Step 4: Image markdown placeholders\n",
    "    image_placeholders = \"\\n\".join([\n",
    "        f\"![image]({img})\" for img in all_images\n",
    "    ]) if all_images else \"No images extracted.\"\n",
    "\n",
    "    # Step 5: Construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "You are an educational content generator. Your task is to create a structured lesson using the extracted text, tables, and images provided below.\n",
    "\n",
    "### Extracted Text:\n",
    "{combined_text}\n",
    "\n",
    "### Extracted Tables (Markdown Format):\n",
    "{tables_markdown}\n",
    "\n",
    "### Source Webpages & Documents:\n",
    "{', '.join(all_sources)}\n",
    "\n",
    "### Extracted Images:\n",
    "Use the following image files at appropriate points in your lesson. Include each one using Markdown syntax like: `![image](imageX.png)`.\n",
    "\n",
    "{image_placeholders}\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 Instructions:\n",
    "- Integrate the **Markdown tables** into the lesson where they add value.\n",
    "- **Include each image** by referencing it at the most relevant point in the content (using the Markdown format).\n",
    "- Write a clear, engaging lesson with sections, subheadings, and explanations.\n",
    "- Briefly describe the images where included, to help learners understand their relevance.\n",
    "\n",
    "Begin generating the lesson below:\n",
    "\"\"\"\n",
    "\n",
    "    # Step 6: Load the model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    # Step 7: Tokenize and check input\n",
    "    encoded = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    input_ids = encoded[\"input_ids\"]\n",
    "\n",
    "    if torch.isnan(input_ids).any() or torch.isinf(input_ids).any():\n",
    "        raise ValueError(\"Input prompt contains NaN or Inf values!\")\n",
    "\n",
    "    # Step 8: Generate only new text\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=1024,  # More room for generation\n",
    "        do_sample=False,      # Greedy decoding for now\n",
    "        temperature=0.7,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    )\n",
    "\n",
    "    # Step 9: Strip the prompt to get only the new tokens\n",
    "    generated_ids = output_ids.sequences[0][input_ids.shape[-1]:]\n",
    "    output = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Debugging: Show intermediate info\n",
    "    print(\"✅ Prompt Length:\", input_ids.shape[-1])\n",
    "    print(\"✅ Tokens Generated:\", len(generated_ids))\n",
    "    print(\"✅ Output Preview:\\n\", output[:500])  # Preview first 500 characters\n",
    "\n",
    "    # Optional: Save to file for debugging\n",
    "    with open(\"lesson_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(output)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b89fe3-2e82-4c62-ae68-3312627f3957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "pdf_path = \"Classification in Machine Learning - Python Geeks.pdf\"\n",
    "\n",
    "# Extract content\n",
    "sources = [pdf_path]\n",
    "text, tables, images, source_refs = extract_all(sources)\n",
    "lesson = generate_lesson_from_extracted_data(sources)\n",
    "print(lesson if lesson.strip() else \"❌ No content was generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "937a9711-4eeb-4aca-a3c7-448f545faa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  3 01:40:13 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A5000               Off | 00000000:18:00.0 Off |                  Off |\n",
      "| 30%   29C    P8              20W / 230W |  24176MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A5000               Off | 00000000:3B:00.0 Off |                  Off |\n",
      "| 30%   29C    P8              19W / 230W |   3741MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A5000               Off | 00000000:86:00.0 Off |                  Off |\n",
      "| 30%   28C    P8              16W / 230W |   2750MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A5000               Off | 00000000:AF:00.0 Off |                  Off |\n",
      "| 30%   29C    P8              24W / 230W |   1893MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      8451      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A    772251      C   /opt/tljh/user/bin/python                   304MiB |\n",
      "|    0   N/A  N/A   1461137      C   /opt/tljh/user/bin/python                   498MiB |\n",
      "|    0   N/A  N/A   1484827      C   /opt/tljh/user/bin/python                  2182MiB |\n",
      "|    0   N/A  N/A   2408136      C   /opt/tljh/user/bin/python                 19488MiB |\n",
      "|    0   N/A  N/A   3105975      C   /opt/tljh/user/bin/python                  1678MiB |\n",
      "|    1   N/A  N/A      8451      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A    957064      C   /opt/tljh/user/bin/python                  2562MiB |\n",
      "|    1   N/A  N/A   1484827      C   /opt/tljh/user/bin/python                  1162MiB |\n",
      "|    2   N/A  N/A      8451      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    2   N/A  N/A    648490      C   /opt/tljh/user/bin/python                   498MiB |\n",
      "|    2   N/A  N/A   1484827      C   /opt/tljh/user/bin/python                  1162MiB |\n",
      "|    2   N/A  N/A   1891485      C   /opt/tljh/user/bin/python                   498MiB |\n",
      "|    2   N/A  N/A   2544035      C   /opt/tljh/user/bin/python                   566MiB |\n",
      "|    3   N/A  N/A      8451      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    3   N/A  N/A    251159      C   /opt/tljh/user/bin/python                   714MiB |\n",
      "|    3   N/A  N/A   1484827      C   /opt/tljh/user/bin/python                  1162MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6987e-7c86-425c-aa31-272e7a4a6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lesson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc588e4-f6be-4f0c-b128-9a6b985c3330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
